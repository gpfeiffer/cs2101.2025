{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "419ec4b9",
   "metadata": {},
   "source": [
    "### Programming for Science and Finance\n",
    "\n",
    "*Prof. Götz Pfeiffer, School of Mathematical and Statistical Sciences, University of Galway*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f4aaf5",
   "metadata": {},
   "source": [
    "# Notebook 8: Programming With Functions II\n",
    "\n",
    "This notebook accompanies **Part II**. You will:\n",
    "\n",
    "* learn how to approximate **derivatives** using finite differences and understand these formulas as convolutions\n",
    "* implement the **trapezoidal rule** and relate it to cumulative sums\n",
    "* use discrete convolution and smoothing kernels to interpret **numerical differentiation and integration**\n",
    "* apply and compare **root-finding** methods, including bisection and Newton’s method\n",
    "* structure numerical algorithms using **classes** to encapsulate solver behavior\n",
    "* solve simple **ordinary differential equations** with Euler’s method\n",
    "* develop a unified view of numerical methods in terms of discrete calculus and iterative schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c136cf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7361510",
   "metadata": {},
   "source": [
    "## Task 1.  Numerical Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70c7508",
   "metadata": {},
   "source": [
    "#### Computing the Derivative of a Function\n",
    "\n",
    "* Recall the **definition of the derivative** of a function $f(x)$:\n",
    "$$\n",
    "f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h}.\n",
    "$$\n",
    "* Thus, for sufficiently small $h$, we get the following approximation of the derivative:\n",
    "$$\n",
    "f'(x) \\approx \\frac{f(x+h) - f(x)}{h}.\n",
    "$$\n",
    "* While it is not entirely clear what \"sufficiently small\" means, this is something we can implement in Python.\n",
    "* As a function `diff` with the following parameters:  \n",
    "  1. the function $f(x)$ that we wish to differentiate,\n",
    "  2. the value of $h$,    \n",
    "  returning a Python function that computes an **approximation** of the derivative $f'(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28772a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(f, h):\n",
    "    def f1(x):\n",
    "        return (f(x+h) - f(x))/h\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94e840",
   "metadata": {},
   "source": [
    "* Warning: if $h$ is too small, floating point rounding errors may dominate the result.\n",
    "* Application to $f(x) = \\frac12 x^2$ with derivative $f'(x) = x$ (and $h = 0.01$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5746af",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x : x**2/2\n",
    "f1 = diff(f, 1/100)\n",
    "[f1(x) for x in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53184c1a",
   "metadata": {},
   "source": [
    "* Using a `lambda` expression, the python code for `diff` can be shortened to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3d2bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff1(f, h):\n",
    "    return lambda x: (f(x+h) - f(x))/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29537690",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x : x**2/2\n",
    "f1 = diff1(f, 1/100)\n",
    "[f1(x) for x in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1a67c9",
   "metadata": {},
   "source": [
    "* Alternatively, the formula\n",
    "  $$\n",
    "  f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x-h)}{2h}\n",
    "  $$\n",
    "  can be used to approximate the derivative of $f$ as\n",
    "  $$\n",
    "  f'(x) \\approx \\frac{f(x+h) - f(x-h)}{2h}\n",
    "  $$\n",
    "  for \"small\" values of `h`.\n",
    "\n",
    "* This can be implemented in a similar way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d683ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff2(f, h):\n",
    "    return lambda x: (f(x+h) - f(x-h))/(2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79ad3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x : x**2/2\n",
    "f1 = diff2(f, 1/100)\n",
    "[f1(x) for x in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1355ccdf",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercises.**\n",
    "\n",
    "1. Write a function `diff3` that uses backward differences $(f(x) - f(x-h))/h$ to approximate\n",
    "   the derivative of $f(x)$.\n",
    "\n",
    "2. Plot and compare the effects of `diff1`, `diff2` and `diff3` on $f(x) = \\frac12x^2$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a359b4",
   "metadata": {},
   "source": [
    "## Task 2.  Differentiation as Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92039278",
   "metadata": {},
   "source": [
    "* In a vectorized version, we often work with a function $f$ in the form of\n",
    "  a **samples list** `yyy` of $N+1$ function values $y_k = f(x_k)$ for $k = 0, \\dots, N$,\n",
    "  where the $x_k$, $k = 0, \\dots, N$ form a list `xxx` often constructed\n",
    "  as\n",
    "  ```python\n",
    "  xxx = np.linspace(a, b, N+1)\n",
    "  ```\n",
    "  (Then the function can be plotted easily as `plt.plot(xxx, yyy)`, where `yyy = f(xxx)`.)  \n",
    "* In such a context\n",
    "  it is natural to set $h = (b-a)/N = x_{k+1} - x_k$, and the (second) formula for the \n",
    "  derivative of $f$ becomes\n",
    "  $$\n",
    "  f'(x_k) \\approx \\frac1{2h}(1 \\cdot y_{k+1} + 0 \\cdot y_k + (-1) \\cdot y_{k-1}).\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a671398",
   "metadata": {},
   "source": [
    "* Recall the convolution product $c = a * b$ of lists $a$ and $b$, where the $k$-th element of the product list $c$ is\n",
    "  $$\n",
    "  c_k = \\sum_{i+j=k} a_i b_j\n",
    "  = a_{-1} b_{k+1} + a_0 b_k + a_1 b_{k-1} + \\dotsm.\n",
    "  $$\n",
    "\n",
    "* On comparison with the previous formula, one sees that **differentiation is convolution** of the samples list `yyy` with a kernel of the form $\\frac1{2h}[1, 0 ,-1]$.  And note that $[1, 0, -1] = [1, 1] * [1, -1]$ is itself a convolution of a smoothing kernel $s = [1,1]$ and a differentiation kernel $d = [1, -1]$.  (In terms of polynomials, think: $(1 + x) (1 - x) = 1 - x^2$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a1965e",
   "metadata": {},
   "source": [
    "* This yields a python function `diff_samples2` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d99902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_samples2(yyy, h):\n",
    "    return np.convolve(yyy, np.array([1, 0, -1])/(2*h), \"same\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbc58e1",
   "metadata": {},
   "source": [
    "* Apply this to $f(x) = \\frac12 x^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc364f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x**2/2\n",
    "xxx = np.linspace(1,10, 11)\n",
    "yyy = f(xxx)\n",
    "diff_samples2(yyy, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c3438d",
   "metadata": {},
   "source": [
    "* Not too bad, except for the endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d3664",
   "metadata": {},
   "source": [
    "* The first differentiation formula could be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9ef81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_samples1(yyy, h):\n",
    "    return np.convolve(yyy, np.array([1, -1])/h, \"same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fe212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_samples1(yyy, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f55ec64",
   "metadata": {},
   "source": [
    "* For more flexibility, we can provide the width `w` of the smoothing kernel as optional parameter, while also fixing the two ends of the interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6410833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_samples(yyy, h, w=2):\n",
    "    ker = np.convolve(np.ones(w), np.array([1, -1]))\n",
    "    yy1 = np.convolve(yyy, ker/(w*h), \"same\")\n",
    "    e = (w+1)//2   # replicate nearest valid value\n",
    "    return np.pad(yy1[e:-e], e, \"edge\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d9ca5c",
   "metadata": {},
   "source": [
    "* Padding with `\"edge\"` replicates nearest valid values to avoid artificial oscillations at the boundaries.\n",
    "* Let's apply this to $f(x) = \\sin(2 \\pi x) + \\frac15 \\cos(10 \\pi x)$ for $x \\in [0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64802705",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: np.sin(2*np.pi*x) + 0.2*np.cos(10*np.pi*x)\n",
    "a, b = 0, 1\n",
    "N = 500\n",
    "xxx = np.linspace(a, b, N+1)\n",
    "h = (b - a) / N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed3596",
   "metadata": {},
   "source": [
    "* We plot $10f(x)$ for the scale, and the approximated $f'(x)$ for 4 different values of `w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc75d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yyy = f(xxx)\n",
    "plt.plot(xxx, 10*yyy, label=\"10f\")\n",
    "\n",
    "for w in range(1, 5):\n",
    "    plt.plot(xxx, diff_samples(yyy, h, w), label=f\"w={w}\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5f4a39",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercises.**\n",
    "\n",
    "1. Implement backward differences $f'(x) \\approx (f(x) - f(x-h))/h$ as `diff_samples3` using a suitable convolution product.\n",
    "\n",
    "2. Apply `diff_samples3` to $f(x) = \\sin(2 \\pi x) + \\frac15 \\cos(10 \\pi x)$ for $x \\in [0, 1]$,\n",
    "   plot the result and compare it with the result of `diff_samples`.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e9affc",
   "metadata": {},
   "source": [
    "## Task 3.  Numerical Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3642ca66",
   "metadata": {},
   "source": [
    "* The **Riemann integral** $I = \\int_a^b f(t)\\, dt$ of a function $f$ over the interval $[a, b]$ is defined as a certain limit of weighted sums of function values.\n",
    "\n",
    "* We can use that weighted sum as approximation to the integral:\n",
    "  $$\n",
    "  I \\approx h \\sum_{k=0}^N f(x_k),\n",
    "  $$\n",
    "  where, as above, $[a, b]$ is subdivided in $N$ intervals of length $h = \\frac{b-a}{N}$.\n",
    "\n",
    "* Alternatively, we can use the **Trapezoidal Rule**:\n",
    "  $$\n",
    "  I \\approx \\frac{b-a}{2N} (f(x_0) + 2f(x_1) + \\dotsm + 2 f(x_{N-1}) + f(x_N))\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1efe264",
   "metadata": {},
   "source": [
    "* With this a function `trapez(f, N)` that takes a function `f` and a number `N` as input and constructs a function `F(a, b)` that can compute an approximation to the definite integral $\\int_a^b f(t)\\, dt$ can be implemented as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e5adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trapez(f, N):\n",
    "    def F(a, b):\n",
    "        xxx = np.linspace(a, b, N+1)\n",
    "        yyy = f(xxx)\n",
    "        return (b-a)/(2*N) * (yyy[0] + yyy[-1] + 2*yyy[1:-1].sum())\n",
    "    return F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32512a9",
   "metadata": {},
   "source": [
    "* **Example.** $f(x) = 2x$ with $\\int_a^b f(t)\\, dt = b^2 - a^2$ and $N = 10$.\n",
    "* Warning: choosing $N$ too small could result in a too coarse approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a746a551",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x : 2*x\n",
    "F = trapez(f, 10)\n",
    "F(0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270d2371",
   "metadata": {},
   "source": [
    "### Integration as Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505cde9a",
   "metadata": {},
   "source": [
    "* Inspecting the formulas for integration one sees that **integration is convolution** of the samples list `yyy` with a kernel of the form $h [1, 1, \\dots, 1]$, or of the form $\\frac{h}{2}[1, 2, 2, \\dots, 2, 1]$.  \n",
    "\n",
    "* Note that $[1, 2, 2, \\dots, 2, 1] = [1, 1] * [1, 1, \\dots, 1]$, the convolution of a **short smoothing kernel** $[1,1]$ of width $w = 2$  and a **long summation kernel** $[1,1,\\dots, 1]$(think: $(1 + x)(1 + x + x^2) = (1 + 2x + 2x^2 + x^3)$).\n",
    "\n",
    "* Convolution with the long summation kernel $[1, 1, \\dots, 1]$ is actually more efficiently\n",
    "computed as a **cumulative sum**. That's what `np.cumsum()` does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123034d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1, 2, 3, 4]\n",
    "print(np.convolve(l, [1,1,1,1])[:len(l)])\n",
    "print(np.cumsum(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f86406",
   "metadata": {},
   "source": [
    "* With these considerations in mind, we can implement integration by convolution\n",
    "  with the width $w$ of the short smoothing kernel as an optional parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804a368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_samples(yyy, h, w=2):\n",
    "    return np.cumsum((h/w) * np.convolve(yyy, np.ones(w)))[:len(yyy)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d48f9ff",
   "metadata": {},
   "source": [
    "* Note how we need to truncate the result to make it the same length as the samples array `yyy`. \n",
    "* Let's apply this to $f(x) = \\sin(2 \\pi x) + \\frac15 \\cos(10 \\pi x)$ for $x \\in [0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712a277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: np.sin(2*np.pi*x) + 0.2*np.cos(10*np.pi*x)\n",
    "\n",
    "a, b, N = 0, 1, 500\n",
    "h = (b - a) / N\n",
    "\n",
    "xxx = np.linspace(a, b, N+1)\n",
    "yyy = f(xxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cc3f1b",
   "metadata": {},
   "source": [
    "* Compute the antiderivative and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93798cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "zzz = integrate_samples(yyy, h)\n",
    "plt.plot(xxx, zzz, label=\"F\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a9bfe1",
   "metadata": {},
   "source": [
    "* Now differentiate the antiderivative and compare the result with the original function $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a993a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xxx, diff_samples(zzz, h), label=\"F'(x)\")\n",
    "plt.plot(xxx, yyy, label=\"f(x)\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb399b96",
   "metadata": {},
   "source": [
    "* Indistinguishable!  That's exactly what the **Fundamental Theorem of Calculus** (FTC) predicts.\n",
    "\n",
    "* Looking at differentiation and integration as convolution, FTC takes this form: \n",
    "  $$\n",
    "  [1,-1] * [1, 1, 1, \\dotsc] = [1],\n",
    "  $$\n",
    "  i.e., the short difference kernel $[1, -1]$ and the infinite summation kernel $[1,1,1,\\dots]$\n",
    "  are each other's **convolution inverses**\n",
    "  (think geometric series: $\\frac{1}{1-x} = \\sum_i x^i$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9323bc",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercises.**\n",
    "\n",
    "1. Write a Python function `antiderivative(f, a, N)` that uses `trapez(f, N)` to approximate the antiderivative $F(x) = \\int_a^x f(t)\\,dt$ of $f(x)$.\n",
    "\n",
    "2. Write a Python function `trapezoid_samples(yyy, h)` that first convolves `yyy` with\n",
    "   the short smoothing kernel $k = \\frac{h}2 [1, 1]$ and then returns the\n",
    "   cumulative sum of the results as the list of antiderivative samples.  Compare the output\n",
    "   with `integrate_samples(yyy, h)` for a test function.\n",
    "\n",
    "3. Sample the function $f(x) = \\sin(10x)+\\frac{2}{5}\\cos(20x)$ on the interval $x \\in [0, \\frac{\\pi}{5}]$ using $N = 20$ subintervals.\n",
    "  Compute the samples `yyy = f(xxx)` and the step size $h$.\n",
    "\n",
    "4. Using `integrate_samples`, compute approximations to the antiderivative\n",
    "   corresponding to window sizes $w = 1, 2, 3$.  Compute the exact antiderivative $F(x)$\n",
    "   with $F(0) = 0$.  Plot all four curves, and discuss how they differ and why.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521513a5",
   "metadata": {},
   "source": [
    "## Task 4.  Root Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e30df6d",
   "metadata": {},
   "source": [
    "A **root** (or **zero**) of a function $f \\colon \\mathbb{R} \\to \\mathbb{R}$\n",
    "is a number $x \\in \\mathbb{R}$ such that $f(x) = 0$.  \n",
    "Often, the equation $f(x) = 0$ cannot be solved for $x$.\n",
    "\n",
    "A **Root Finding Algorithm** can approximate a root of $f$, without a guarantee to find **all** roots of $f$.  \n",
    "There are several such algorithms, with varying degrees of sophistication.\n",
    "We discuss two simple strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f053093e",
   "metadata": {},
   "source": [
    "### Bisection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7753b9d0",
   "metadata": {},
   "source": [
    "For a given **tolerance** $\\epsilon$, and a maximum number $M$ of iterations,\n",
    "\n",
    "* **Initially**, pick an interval $[a, b]$ such that $f(a) \\cdot f(b) < 0$, i.e., $f(a)$ and $f(b)$ have **opposite signs**.  (Then by the [IVT](https://en.wikipedia.org/wiki/Intermediate_value_theorem), there exists at least one $x \\in (a, b)$ with $f(x) = 0$.)\n",
    "\n",
    "* **Repeat** at most $M$ times:\n",
    "  * $c = \\frac{a + b}{2}$ (midpoint)\n",
    "  * replace either $a$ or $b$ by $c$ so that $f(a) \\cdot f(b) < 0$\n",
    "\n",
    "  until $|f(c)| < \\epsilon$  \n",
    "* **Return** $c$ as an approximation to a root of $f$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1afdf5",
   "metadata": {},
   "source": [
    "This method is simple but slow, as it needs approximately $\\log_2 \\frac{b-a}{\\epsilon}$ steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6137644a",
   "metadata": {},
   "source": [
    "### Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640840a4",
   "metadata": {},
   "source": [
    "The [Newton-Raphson Method](https://en.wikipedia.org/wiki/Newton%27s_method) uses the derivative $f'(x)$ to locate a zero of $f(x)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd51951f",
   "metadata": {},
   "source": [
    "For a given **tolerance** $\\epsilon$, and a maximum number $M$ of iterations,\n",
    "\n",
    "* **Initially**, guess a value $x$.\n",
    "\n",
    "* **Repeat** at most $M$ times:\n",
    "  * replace $x$ by $x - f(x)/f'(x)$\n",
    "\n",
    "  until $|f(x)| < \\epsilon$\n",
    "\n",
    "* **Return** $x$ as an approximation to a root of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1c3209",
   "metadata": {},
   "source": [
    "This method converges quadratically, if the initial guess $x$ is close enough to the actual root."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e9ed14",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercises.**\n",
    "\n",
    "1. Implement the bisection method as a Python function `bisection`.\n",
    "\n",
    "2. Apply your `bisection` function to solve $x^2 = 2$.\n",
    "\n",
    "1. Implement the Newton method as a Python function `newton`.\n",
    "\n",
    "2. Apply your `newton` function to solve $x^2 = 3$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab178ac",
   "metadata": {},
   "source": [
    "## Task 5. A Root Finding Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473ea559",
   "metadata": {},
   "source": [
    "Seeing the above two root finding methods have a lot of structure in common, it might be a good idea to implement both (and perhaps other root finding methods) with a common API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a257147",
   "metadata": {},
   "source": [
    "Both methods work with more or less the same setup, consisting of:\n",
    "\n",
    "* a function $f(x)$,\n",
    "* its derivative $f'(x)$ (optional)\n",
    "* a tolerance $\\epsilon$ (default value $10^{-6}$)\n",
    "* a maximal number $M$ of iterations (default value $99$).\n",
    "\n",
    "With this setup, one can build a function `solve` that finds a root, starting with one or two initial values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b447a04f",
   "metadata": {},
   "source": [
    "For an implementation, we choose the following **design**. We define an abstract parent **class**\n",
    "`RootFinder` for the general setup.  Different strategies can then be defined as **subclasses** of `RootFinder` and only need to implement the `solve` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f2a827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RootFinder:\n",
    "    def __init__(self, f, f1=None, eps=1e-6, M=99):\n",
    "        self.f = f\n",
    "        self.f1 = f1\n",
    "        self.eps = eps\n",
    "        self.M = M\n",
    "\n",
    "    def solve(self, a, b= None):\n",
    "        raise NotImplementedError(\"Subclass must implement solve().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd742b9",
   "metadata": {},
   "source": [
    "The `solve` method in the parent class serves only as a placeholder.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f133a0a0",
   "metadata": {},
   "source": [
    "The Bisection method as a subclass `BisectionSolver` inherits the `__init__` method and\n",
    "hence needs no implementatoin of its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f66dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BisectionSolver(RootFinder):\n",
    "    def solve(self, a, b):  # overwrite parent's method\n",
    "        f = self.f\n",
    "        assert f(a) * f(b) < 0, \"end points must have opposite signs\"\n",
    "        for i in range(self.M):\n",
    "            c = (a+b)/2\n",
    "            if abs(f(c)) < self.eps:\n",
    "                return c\n",
    "            if f(a) * f(c) < 0:\n",
    "                b = c\n",
    "            else:\n",
    "                a = c\n",
    "        print(\"Warning: maximum iterations reached.\")\n",
    "        return c  # now i >= M\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370aea12",
   "metadata": {},
   "source": [
    "Application to $x^2 = 2$, i.e., find $\\sqrt{2}$:\n",
    "\n",
    "* $f(x) = x^2 - 2$.\n",
    "* $[a, b] = [1, 2]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c6f9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = BisectionSolver(lambda x: x**2 - 2)\n",
    "x = solver.solve(1, 2)\n",
    "print(x, x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffce98e2",
   "metadata": {},
   "source": [
    "The Newton method as a subclass `NewtonSolver` inherits the `__init__` method and\n",
    "hence needs no implementatoin of its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1726eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewtonSolver(RootFinder):\n",
    "    def solve(self, a, b=None):\n",
    "        f, f1 = self.f, self.f1\n",
    "        assert f1 is not None, \"f1 needed\"\n",
    "        for i in range(self.M):\n",
    "            y = f(a)\n",
    "            if abs(y) < self.eps:\n",
    "                return a\n",
    "            a -= y/f1(a)\n",
    "        print(\"Warning: maximum iterations reached.\")\n",
    "        return a  # now i >= M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b70ed57",
   "metadata": {},
   "source": [
    "Application to $x^2 = 3$, i.e., find $\\sqrt{3}$:\n",
    "\n",
    "* $f(x) = x^2 - 3$\n",
    "* $f'(x) = 2x$\n",
    "* $a = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb314e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = NewtonSolver(lambda x: x**2 - 3, lambda x: 2*x)\n",
    "x = solver.solve(1)\n",
    "print(x, x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c695a58",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercises.**\n",
    "1. Find a root of $f(x) = x^3 - x - 2$ between $1$ and $2$.\n",
    "\n",
    "1. Solve $\\cos x = x$ (with $[a, b] = [0, 1]$).\n",
    "\n",
    "1. Solve $x e^x = 2$ (with $[a, b] = [0, 1]$).\n",
    "\n",
    "3. Implement a Newton solver that computes $f'(x)$ from $f(x)$ rather than using an explict argument.\n",
    "\n",
    "4. Modify the solver classes by renaming `solve` to `__call__`.  Why would that be a good idea?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7df94c4",
   "metadata": {},
   "source": [
    "## Task 6. Solving Differential Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8f9ed1",
   "metadata": {},
   "source": [
    "A **first order initial value problem** (IVP)  has the form\n",
    "$$\n",
    "y'(t) = f(t, y(t)),\\\\\n",
    "y(t_0) = y_0,\n",
    "$$\n",
    "i.e., a first order **differential equation** (expressing the derivative of a function $y(t)$ in terms of $y$ and $t$) and an **initial condition** (specifying the value of $y$ at a particular point).   The goal is to find a function $y(t)$ that satisfies both the differential equation and the initial condition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995a2d6b",
   "metadata": {},
   "source": [
    "### Euler's Method\n",
    "\n",
    "A simple method for approximating a solution to an IVP uses the approximation to the derivative\n",
    "$$\n",
    "y'(t) \\approx \\frac{y(t + h) - y(t)}{h}\n",
    "$$\n",
    "Setting\n",
    "$$\n",
    "t_n = t_{n-1} + h,\\quad n > 0,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "y_n = y(t_n),\n",
    "$$\n",
    "we can rewrite\n",
    "$$\n",
    "f(t_0, y_0) = y'(t_0) \\approx \\frac{y(t_1) - y(t_0)}{h}\n",
    "= \\frac{y_1 - y_0}{h}\n",
    "$$\n",
    "as an approximation\n",
    "$$\n",
    "y_1 \\approx y_0 + h \\cdot f(t_0, y_0)\n",
    "$$\n",
    "and then, continuing in that fashion:\n",
    "$$\n",
    "y_{n+1} \\approx y_n + h \\cdot f(t_n, y_n).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b68347",
   "metadata": {},
   "source": [
    "This process can be implemented as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a05b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler(f, t0, y0, h):\n",
    "    def approx(n):\n",
    "        t, y = t0, y0\n",
    "        for i in range(n):\n",
    "            y += h * f(t, y)\n",
    "            t += h\n",
    "        return y\n",
    "    return approx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1cf08f",
   "metadata": {},
   "source": [
    "**Example.** Consider\n",
    "$$\n",
    "y' = 2y +t^2 - t \\\\ y(0) = 1.\n",
    "$$\n",
    "with $h = 0.01$, say.\n",
    "\n",
    "An exact solution to this IVP is $y = e^{2t} - t^2/2$, as can be easily verified:\n",
    "clearly, $y(0) = 1$, and $y' = 2e^{2t} - t$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d415b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = euler(lambda t, y: 2*y + t**2 - t, 0, 1, 0.01)\n",
    "e(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f594da8",
   "metadata": {},
   "source": [
    "Plot the estimate and the exact solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d98bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = np.linspace(0, 1, 101)\n",
    "y = lambda t: np.exp(2*t) - t**2/2  # exact solution\n",
    "plt.plot(ttt, y(ttt), label=\"exact\")\n",
    "plt.plot(ttt, [e(n) for n in range(101)], label=\"estimate\")\n",
    "_ = plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33804be",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercises.**\n",
    "1. Solve $y' = -2y$, $y(0) = 100$ (with $h = 1$ and $N = 100$) and compare the result with the exact solution $y(t) = 100 e^{-2t}$.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de67040f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* introduced **finite difference methods** for numerical differentiation\n",
    "* interpreted differentiation as **convolution with short kernels**\n",
    "* implemented **numerical integration**, including the trapezoidal rule\n",
    "* connected integration to **cumulative sums and convolution**\n",
    "* applied two core **root-finding** methods: bisection and Newton’s method\n",
    "* organized solvers into **classes** to encapsulate algorithms\n",
    "* solved simple **ordinary differential equations** using Euler’s method\n",
    "* highlighted how differentiation, integration, and iteration form the basis of numerical computation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
